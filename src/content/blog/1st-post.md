---
title: "Project 1: Persephone’s Pomegranate (My AI Music Channel Experiment)"
date: "2026-02-11"
excerpt: "How I built generated music using AI and other modern web tools."
author: "B.D.Higgins"
---

Project 1: Persephone’s Pomegranate (My AI Music Channel Experiment)
I didn’t start with some serious AI startup.
My first real project was a simple YouTube music channel: I used Suno to make songs and a mix of Canva, CapCut, and Audacity to turn them into videos. It was clunky, messy, and exactly what I needed to start shipping instead of just thinking.

You can see it here if you want to scroll from the ugly beginnings to the slightly less ugly present:
[Persephone’s Pomegranate on YouTube] (https://youtube.com/@persephonespomegranate-gt3jk)

Why I Started the Channel
I had made some music I really liked using AI, and I wanted to share it with people. 
The goal wasn’t to blow up or “become an artist.” The goal was to learn how to user A.i. as a creative tool and share what I created with everyone else. A lot of people didn't like it just because A.i. was involved. A lot of people have
things they want to say to the world but not the talent to express themselves in a manner that catches any attention. For me it's not about having or not having talent. It's a chance to be heard. Although, I still havent been heard much.
but that's okay. They say Ai reflects back whatever you give it. So when I tell chat gpt a story about my life and have it write a few verses about it, and sometimes tweak it here and there..well isn't that me just being reflected ? I really am grateful for all the help they have given me in all aspects of my life and i couldnt have gotten done or even learned what I have learned the past 2-3 years without Ais automation, assistance, and patience its had with me. 
In any case.. 

Tools I Used
For this project, the stack was pretty simple:

[Suno] (www.suno.com) for generating the actual songs (lyrics, melodies, full tracks).

[Canva] (www.canva.com) for my first music videos, basically just stock‑photo slideshows.

[CapCut] (www.capcut.com) for “real” video editing once I got brave enough to touch timelines and layers.

[Audacity] (https://sourceforge.net/projects/audacity/) for slicing and cleaning audio when I started doing more detailed edits.

That was enough to go from “I have nothing” to “I have an actual channel with multiple tracks live.”

The Canva Slideshow Era
The first videos were as basic as it gets.

My workflow looked like this:

Generate a track in Suno using a simple prompt: genre, vibe, maybe a theme.

Open Canva and drop in a bunch of stock photos that vaguely matched the mood.

Arrange them on a timeline, add simple transitions, and stretch the slideshow to match the length of the song.

Export, upload to YouTube, write a quick title and description, hit publish.

They were literally AI‑generated songs playing over a stock‑photo slideshow. No fancy animations, no storytelling—just proof that I could ship something instead of leaving it in my head.

Discovering CapCut (and Being Totally Overwhelmed)
At some point, Canva slideshows stopped being enough.
I kept seeing people mention CapCut, so I installed it, opened it… and immediately wanted to close it. Timelines, layers, effects, keyframes—I’d never touched a real editor before, so it felt like sitting in a plane cockpit.

I didn’t learn it from courses or YouTube tutorials. I just clicked around, dragged things, broke things, and slowly started to understand what each button did. Full FAFO energy.

Over time, a few things shifted:

Early on, I was doing basic cuts, simple text, maybe one transition.

Later, I started lining edits up with the beat, using more intentional visuals, and caring about pacing.

Eventually, CapCut stopped feeling like a wall of buttons and more like a tool I could bend into a pipeline.

Most of that improvement came from brute‑forcing my way through CapCut and repeating the process over and over. No course, no structured plan—just messing around until the videos in my head started to match what was on the screen.

When I Tried to Fix the Audio and Made It Worse
Then I decided to “get serious” about the audio.

I pulled Audacity into the mix to clean up my voice tracks. I ended up with two versions of the same audio:

One “good” cleaned‑up audio track from Audacity sitting on its own track in CapCut.

Another copy of the audio still attached to the video clips, which I was cutting into tiny 5‑second chunks to drive the lipsync.

The plan sounded smart in my head: use the chopped‑up audio to get the lipsync perfect, then mute the video’s sound and let the clean Audacity track be what people actually hear.

The way I broke it is kind of funny in hindsight. I was cutting the video and that attached audio into pieces in CapCut, exporting those little 5‑second segments, and stitching them back together. The problem showed up when I started lining everything up again:

The sliced audio that was baked into the video slowly drifted away from the “good” Audacity track I’d left on the timeline.

I basically ended up with two almost‑identical audio timelines that were just far enough out of sync to ruin the lipsync.

If I muted the video’s sound like I’d planned, the lipsync fell apart. If I kept it, the two tracks didn’t match anyway.

So instead of one clean fix, I’d created my own nightmare by trying to use two audio tracks for one mouth.

Under the hood, this is a classic “audio drift” problem: different apps, slightly different settings, and what should be identical timing slowly pulls apart over the length of the video.

That whole mess taught me two things:

Don’t overcomplicate your workflow before you have to.

When audio randomly goes out of sync, check the boring stuff (sample rates and export settings) before you assume you “just suck” at editing.

ChatGPT eventually helped me figure out what was actually going wrong with the audio. But the real lesson wasn’t just about settings. Sometimes when things aren’t going your way, you either have to walk away and come back later, or just go with the flow of the tools and apps you’re using. It might not match your exact vision, and that’s where you have to channel the great Bob Ross: your job is to turn those “happy accidents” into art. Don’t force every pixel to match the picture in your head—flow with what you’re given and make something cool out of it instead.
​
​

The Burning Man Tutorial That Leveled Me Up
In the middle of all this, I followed a random tutorial on how to make one of those “Burning Man” music videos that were everywhere for about three weeks one summer. The trend itself came and went fast, but the tutorial accidentally taught me something way more important than just “how to make a Burning Man edit”.

It showed me how to break a video into:

A storyboard

Scenes

Individual shots and moments

Instead of just throwing clips on a timeline and hoping it felt cool, I started mapping things out: intro shots, main sections, transitions, and an ending. Basically, I went from “slideshow with vibes” to “there’s a rough plan here.”




That was the first time I realized music videos aren’t just random visuals with a song underneath. They’re built from a sequence of planned shots, and a simple storyboard makes the whole process way less chaotic and way more intentional.

A Channel That Couldn’t Pick a Genre
The channel itself struggled to find an identity.
I love all kinds of music, so whatever genre I felt like creating that day Metal, Country, Rock, EDM, even some R&B—I just made it and uploaded it. From a “YouTube growth” point of view, that’s not ideal; most advice says to pick one lane and stay there so the algorithm and audience know what to expect.

But for this first project, the point wasn’t to be a perfectly branded artist. The point was to use AI as a sandbox, jump between styles, and learn how to produce and ship consistently. 

When Effort and Results Didn’t Match
The songs I thought were really good… nobody cared.
Even after about two years, that channel is sitting at around 22 subscribers, and a lot of the tracks I was proud of,  barely moved the needle. Music videos are hard work, and I have to really like a song before I sink hours into editing, so that disconnect hits extra hard.

On top of the music, I experimented with some Hollywood history / “patterns that repeat” shorts on the same channel—breaking down character archetypes across sitcoms and how they link back to older shows. That content probably doesn’t “belong” on an AI music channel, but of course those were some of my most‑viewed videos. The algorithm and viewers seemed to grab onto that way faster than the songs I cared most about.
​

One track didn’t get any views for the first two weeks. After eight months it only had around 100 views (which is actually high for that channel)… and then out of nowhere, in the last month it picked up about 382 views and is now responsible for roughly half of my subscribers. The wild part? The video itself was one of the easiest I ever made: an animated image I made in Meta, copied into CapCut, duplicated, reversed, and looped all the way through the song. Minimal effort, maximum performance.
​

Meanwhile, my Burning Man storyboard video—the one I carefully planned out—has maybe around 50 views. That’s when it really clicked: on YouTube, hard work does not automatically equal better results. Topic, timing, and format often matter more than how many hours you threw at the timeline.
​

Spinning Up a 99% AI Cooking Channel
That realization pushed me to start a second channel I wanted to keep about 99% AI‑made, just to see what would happen if I leaned all the way into automation.

I still wasn’t sure what niche to pick, so I went simple: recipes. I copied recipes into CapCut’s new AI video maker and told it to “make a cooking video with this recipe.” Sometimes the text‑to‑speech got a word wrong and I had to fix it, but once I found a voice I liked, a background music track, and a caption font I was happy with, I just reused the same combo for every video.
​

In less than a week, that new AI cooking channel pulled about 18 subscribers and roughly 4,000 views—while my two‑year‑old music channel was sitting around 20 subscribers and maybe 6–7k views total, most of which came from the Hollywood history shorts, not the music itself.

Same platform, same person, way less effort per video… totally different outcome.

What I Learned from All This
Between Persephone’s Pomegranate and the AI recipe channel, YouTube taught me a few brutal truths fast:

The videos I work hardest on won’t always be the ones people care about.

Simple ideas can outperform complex edits.

Sometimes the algorithm likes my weird side quests more than the main project.
​

My job isn’t to control all of that.
My job is to keep experimenting with AI, keep shipping, and turn as many “happy accidents” as possible into something people actually enjoy.

If you’re thinking about starting your own AI‑driven music or content channel and want help with prompts, tools, or workflow—or just want a second brain on your experiments—reach out and let’s build something.